<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/w6BP7vs60TZ0HkHnY561nLODvqk</id>
  <title>arXiv Query: search_query=all:cs.AI&amp;id_list=&amp;start=0&amp;max_results=20</title>
  <updated>2026-01-03T03:49:09Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:cs.AI&amp;start=0&amp;max_results=20&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>20</opensearch:itemsPerPage>
  <opensearch:totalResults>156470</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/cs/9909009v1</id>
    <title>The Rough Guide to Constraint Propagation</title>
    <updated>1999-09-08T13:50:01Z</updated>
    <link href="https://arxiv.org/abs/cs/9909009v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/cs/9909009v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In particular, using the notions commutativity and semi-commutativity, we show how the well-known AC-3, PC-2, DAC and DPC algorithms are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt, cs.AI/9811024.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <published>1999-09-08T13:50:01Z</published>
    <arxiv:comment>23 pages. To appear in the Proc. 5th International Conference on Principles and Practice of Constraint Programming as an invited talk</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Krzysztof R. Apt</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612057v1</id>
    <title>Conscious Intelligent Systems - Part II - Mind, Thought, Language and Understanding</title>
    <updated>2006-12-09T17:28:24Z</updated>
    <link href="https://arxiv.org/abs/cs/0612057v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/cs/0612057v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  This is the second part of a paper on Conscious Intelligent Systems. We use the understanding gained in the first part (Conscious Intelligent Systems Part 1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the presence of mind affects understanding and intelligent systems; we see that the presence of mind necessitates language. The rise of language in turn has important effects on understanding. We discuss the humanoid question and how the question of self-consciousness (and by association mind/thought/language) would affect humanoids too.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2006-12-09T17:28:24Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>U. Gayathree</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203003v1</id>
    <title>Deductive Nonmonotonic Inference Operations: Antitonic Representations</title>
    <updated>2002-03-01T11:20:59Z</updated>
    <link href="https://arxiv.org/abs/cs/0203003v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/cs/0203003v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  We provide a characterization of those nonmonotonic inference operations C for which C(X) may be described as the set of all logical consequences of X together with some set of additional assumptions S(X) that depends anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset of S(X)). The operations represented are exactly characterized in terms of properties most of which have been studied in Freund-Lehmann(cs.AI/0202031). Similar characterizations of right-absorbing and cumulative operations are also provided. For cumulative operations, our results fit in closely with those of Freund. We then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties. Our results provide a satisfactory notion of pseudo-compactness, generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations. They also provide an alternative, more elegant and more general, proof of the existence of an infinitary deductive extension for any finitary deductive operation (Theorem 7.9 of Freund-Lehmann).</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2002-03-01T11:20:59Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <arxiv:journal_ref>Journal of Logic and Computation, 5(1) (1995) pp. 111-122</arxiv:journal_ref>
    <author>
      <name>Yuri Kaluzhny</name>
    </author>
    <author>
      <name>Daniel Lehmann</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702170v1</id>
    <title>Generic Global Constraints based on MDDs</title>
    <updated>2007-02-28T15:32:48Z</updated>
    <link href="https://arxiv.org/abs/cs/0702170v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/cs/0702170v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>  Constraint Programming (CP) has been successfully applied to both constraint satisfaction and constraint optimization problems. A wide variety of specialized global constraints provide critical assistance in achieving a good model that can take advantage of the structure of the problem in the search for a solution. However, a key outstanding issue is the representation of 'ad-hoc' constraints that do not have an inherent combinatorial nature, and hence are not modeled well using narrowly specialized global constraints. We attempt to address this issue by considering a hybrid of search and compilation. Specifically we suggest the use of Reduced Ordered Multi-Valued Decision Diagrams (ROMDDs) as the supporting data structure for a generic global constraint. We give an algorithm for maintaining generalized arc consistency (GAC) on this constraint that amortizes the cost of the GAC computation over a root-to-leaf path in the search tree without requiring asymptotically more space than used for the MDD. Furthermore we present an approach for incrementally maintaining the reduced property of the MDD during the search, and show how this can be used for providing domain entailment detection. Finally we discuss how to apply our approach to other similar data structures such as AOMDDs and Case DAGs. The technique used can be seen as an extension of the GAC algorithm for the regular language constraint on finite length input.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2007-02-28T15:32:48Z</published>
    <arxiv:comment>Preliminary 15 pages version of the tech-report cs.AI/0611141</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Peter Tiedemann</name>
    </author>
    <author>
      <name>Henrik Reif Andersen</name>
    </author>
    <author>
      <name>Rasmus Pagh</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.03545v1</id>
    <title>Is there really a Citation Age Bias in NLP?</title>
    <updated>2024-01-07T17:12:08Z</updated>
    <link href="https://arxiv.org/abs/2401.03545v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2401.03545v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Citations are a key ingredient of scientific research to relate a paper to others published in the community. Recently, it has been noted that there is a citation age bias in the Natural Language Processing (NLP) community, one of the currently fastest growing AI subfields, in that the mean age of the bibliography of NLP papers has become ever younger in the last few years, leading to `citation amnesia' in which older knowledge is increasingly forgotten. In this work, we put such claims into perspective by analyzing the bibliography of $\sim$300k papers across 15 different scientific fields submitted to the popular preprint server Arxiv in the time period from 2013 to 2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG) have similar trends of citation amnesia, in which the age of the bibliography has roughly halved in the last 10 years (from above 12 in 2013 to below 7 in 2022), on average. Rather than diagnosing this as a citation age bias in the NLP community, we believe this pattern is an artefact of the dynamics of these research fields, in which new knowledge is produced in ever shorter time intervals.</summary>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-01-07T17:12:08Z</published>
    <arxiv:primary_category term="cs.DL"/>
    <author>
      <name>Hoa Nguyen</name>
    </author>
    <author>
      <name>Steffen Eger</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12121v1</id>
    <title>NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers?</title>
    <updated>2024-12-02T22:10:38Z</updated>
    <link href="https://arxiv.org/abs/2412.12121v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2412.12121v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The NLLG (Natural Language Learning &amp; Generation) arXiv reports assist in navigating the rapidly evolving landscape of NLP and AI research across cs.CL, cs.CV, cs.AI, and cs.LG categories. This fourth installment captures a transformative period in AI history - from January 1, 2023, following ChatGPT's debut, through September 30, 2024. Our analysis reveals substantial new developments in the field - with 45% of the top 40 most-cited papers being new entries since our last report eight months ago and offers insights into emerging trends and major breakthroughs, such as novel multimodal architectures, including diffusion and state space models. Natural Language Processing (NLP; cs.CL) remains the dominant main category in the list of our top-40 papers but its dominance is on the decline in favor of Computer vision (cs.CV) and general machine learning (cs.LG). This report also presents novel findings on the integration of generative AI in academic writing, documenting its increasing adoption since 2022 while revealing an intriguing pattern: top-cited papers show notably fewer markers of AI-generated content compared to random samples. Furthermore, we track the evolution of AI-associated language, identifying declining trends in previously common indicators such as "delve".</summary>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-12-02T22:10:38Z</published>
    <arxiv:primary_category term="cs.DL"/>
    <author>
      <name>Christoph Leiter</name>
    </author>
    <author>
      <name>Jonas Belouadi</name>
    </author>
    <author>
      <name>Yanran Chen</name>
    </author>
    <author>
      <name>Ran Zhang</name>
    </author>
    <author>
      <name>Daniil Larionov</name>
    </author>
    <author>
      <name>Aida Kostikova</name>
    </author>
    <author>
      <name>Steffen Eger</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2351v1</id>
    <title>Elementos de ingeniería de explotación de la información aplicados a la investigación tributaria fiscal</title>
    <updated>2013-09-10T00:42:05Z</updated>
    <link href="https://arxiv.org/abs/1309.2351v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1309.2351v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>By introducing elements of information mining to tax analysis, by means of data mining software and advanced computational concepts of artificial intelligence, the problem of tax evader's crime against public property has been addressed. Through an empirical approach from a hypothetical case of use, induction algorithms, neural networks and bayesian networks are applied to determine the feasibility of its heuristic application by the tax public administrator. Different strategies are explored to facilitate the work of local and regional federal tax inspectors, considering their limited computational capabilities, but equally effective for those social scientist committed to handcrafting tax research.
  -----
  Apresentando a introdução de elementos de exploração de informações para análise fiscal, por meio de software de mineração de dados e conceitos avançados computacionais de inteligência artificial, foi abordado o problema do crime de sonegador fiscal contra o patrimônio público. Através de uma abordagem empírica a partir de um caso hipotético de uso, os algoritmos de indução, redes neurais e redes bayesianas são aplicados para determinar a viabilidade de sua aplicação heurística pelo administrador público tributário. Diferentes estratégias são exploradas para facilitar o trabalho dos inspectores tributários federais locais e regionais, tendo em conta as suas capacidades computacionais limitados, mas igualmente eficaz para aqueles cientista social comprometido com a investigação fiscal.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2013-09-10T00:42:05Z</published>
    <arxiv:comment>30 pages, 7 figures, written in Castilian, Artificial Intelligence (cs.AI), Computers and Society (cs.CY)</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Rodrigo Lopez-Pablos</name>
      <arxiv:affiliation>Universidad Nacional de La Matanza y Universidad Tecnológica Nacional</arxiv:affiliation>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.02540v2</id>
    <title>Top-down Automated Theorem Proving (Notes for Sir Timothy)</title>
    <updated>2023-08-08T08:17:52Z</updated>
    <link href="https://arxiv.org/abs/2308.02540v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2308.02540v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We describe a "top down" approach for automated theorem proving (ATP). Researchers might usefully investigate the forms of the theorems mathematicians use in practice, carefully examine how they differ and are proved in practice, and code all relevant domain concepts. These concepts encode a large portion of the knowledge in any domain. Furthermore, researchers should write programs that produce proofs of the kind that human mathematicians write (and publish); this means proofs that might sometimes have mistakes; and this means making inferences that are sometimes invalid.
  This approach is meant to contrast with the historically dominant "bottom up" approach: coding fundamental types (typically sets), axioms and rules for (valid) inference, and building up from this foundation to the theorems of mathematical practice and to their outstanding questions. It is an important fact that the actual proofs that mathematicians publish in math journals do not look like the formalized proofs of Russell &amp; Whitehead's Principia Mathematica (or modern computer systems like Lean that automate some of this formalization). We believe some "lack of rigor" (in mathematical practice) is human-like, and can and should be leveraged for ATP.</summary>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-08-01T14:03:40Z</published>
    <arxiv:comment>Cross list with cs.AI</arxiv:comment>
    <arxiv:primary_category term="cs.LO"/>
    <author>
      <name>C. E. Larson</name>
    </author>
    <author>
      <name>N. Van Cleemput</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307048v2</id>
    <title>Integrating cardinal direction relations and other orientation relations in Qualitative Spatial Reasoning</title>
    <updated>2004-10-05T15:45:31Z</updated>
    <link href="https://arxiv.org/abs/cs/0307048v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/cs/0307048v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>  We propose a calculus integrating two calculi well-known in Qualitative Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus, and a coarser version of Freksa's relative orientation calculus. An original constraint propagation procedure is presented, which implements the interaction between the two integrated calculi. The importance of taking into account the interaction is shown with a real example providing an inconsistent knowledge base, whose inconsistency (a) cannot be detected by reasoning separately about each of the two components of the knowledge, just because, taken separately, each is consistent, but (b) is detected by the proposed algorithm, thanks to the interaction knowledge propagated from each of the two compnents to the other.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2003-07-21T13:03:19Z</published>
    <arxiv:comment>Includes new material, such as a section on the use of the work in the concrete domain of the ALC(D) spatio-temporalisation defined in http://arXiv.org/abs/cs.AI/0307040</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Amar Isli</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.22628v1</id>
    <title>Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks</title>
    <updated>2025-10-26T11:19:47Z</updated>
    <link href="https://arxiv.org/abs/2510.22628v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2510.22628v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper presents a real-time modular defense system named Sentra-Guard. The system detects and mitigates jailbreak and prompt injection attacks targeting large language models (LLMs). The framework uses a hybrid architecture with FAISS-indexed SBERT embedding representations that capture the semantic meaning of prompts, combined with fine-tuned transformer classifiers, which are machine learning models specialized for distinguishing between benign and adversarial language inputs. It identifies adversarial prompts in both direct and obfuscated attack vectors. A core innovation is the classifier-retriever fusion module, which dynamically computes context-aware risk scores that estimate how likely a prompt is to be adversarial based on its content and context. The framework ensures multilingual resilience with a language-agnostic preprocessing layer. This component automatically translates non-English prompts into English for semantic evaluation, enabling consistent detection across over 100 languages. The system includes a HITL feedback loop, where decisions made by the automated system are reviewed by human experts for continual learning and rapid adaptation under adversarial pressure. Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and malicious prompts, enhancing detection reliability and reducing false positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible with diverse LLM backends. Its modular design supports scalable deployment in both commercial and open-source environments. The system establishes a new state-of-the-art in adversarial LLM defense.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-10-26T11:19:47Z</published>
    <arxiv:comment>11 pages, 5 figures. Preprint version under review in the area of Artificial Intelligence (cs.AI)</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Md. Mehedi Hasan</name>
    </author>
    <author>
      <name>Ziaur Rahman</name>
    </author>
    <author>
      <name>Rafid Mostafiz</name>
    </author>
    <author>
      <name>Md. Abir Hossain</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2508.16696v1</id>
    <title>DecoMind: A Generative AI System for Personalized Interior Design Layouts</title>
    <updated>2025-08-22T00:01:48Z</updated>
    <link href="https://arxiv.org/abs/2508.16696v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2508.16696v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper introduces a system for generating interior design layouts based on user inputs, such as room type, style, and furniture preferences. CLIP extracts relevant furniture from a dataset, and a layout that contains furniture and a prompt are fed to Stable Diffusion with ControlNet to generate a design that incorporates the selected furniture. The design is then evaluated by classifiers to ensure alignment with the user's inputs, offering an automated solution for realistic interior design.</summary>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-08-22T00:01:48Z</published>
    <arxiv:comment>~7 pages; ~32 figures; compiled with pdfLaTeX. Primary category: cs.CV. (Secondary: cs.AI)</arxiv:comment>
    <arxiv:primary_category term="cs.GR"/>
    <author>
      <name>Reema Alshehri</name>
    </author>
    <author>
      <name>Rawan Alotaibi</name>
    </author>
    <author>
      <name>Leen Almasri</name>
    </author>
    <author>
      <name>Rawan Altaweel</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.11574v1</id>
    <title>A global analysis of metrics used for measuring performance in natural language processing</title>
    <updated>2022-04-25T11:41:50Z</updated>
    <link href="https://arxiv.org/abs/2204.11574v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2204.11574v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Measuring the performance of natural language processing models is challenging. Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages. In the past 15 years, a wide range of alternative metrics have been proposed. However, it is unclear to what extent this has had an impact on NLP benchmarking efforts. Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing. We curated, mapped and systematized more than 3500 machine learning model performance results from the open repository 'Papers with Code' to enable a global and comprehensive analysis. Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models' performance. Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-04-25T11:41:50Z</published>
    <arxiv:comment>"NLP Power" workshop at ACL 2022. This work is based on a previous arXiv submission: arXiv:2008.02577 [cs.AI]</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Kathrin Blagec</name>
    </author>
    <author>
      <name>Georg Dorffner</name>
    </author>
    <author>
      <name>Milad Moradi</name>
    </author>
    <author>
      <name>Simon Ott</name>
    </author>
    <author>
      <name>Matthias Samwald</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2509.12229v1</id>
    <title>Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study</title>
    <updated>2025-09-07T21:41:14Z</updated>
    <link href="https://arxiv.org/abs/2509.12229v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2509.12229v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Fine-tuning large language models (LLMs) with parameter-efficient techniques such as LoRA and QLoRA has enabled adaptation of foundation models on modest hardware. Yet the efficiency of such training on consumer-grade GPUs, especially under strict 8 GB VRAM limits, remains underexplored. We present a controlled profiling study of LoRA/QLoRA fine-tuning using the Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three representative configurations, we systematically vary batch size, sequence length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16). We report throughput (tokens/s), time per 10k tokens, and VRAM footprint, alongside energy estimates derived from GPU board power limits. Our results show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500 tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB constraints, sequence lengths up to 2048 tokens were feasible using parameter-efficient strategies. To our knowledge, this is the first systematic case study of LLM fine- tuning efficiency on consumer GPUs, providing reproducible benchmarks and practical guidelines for resource-constrained researchers and practitioners.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-09-07T21:41:14Z</published>
    <arxiv:comment>8 pages, 3 figures, 2 tables. Primary category: cs.LG (Machine Learning); secondary: cs.AI (Artificial Intelligence). LaTeX source with figures included</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>MSR Avinash</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02565v1</id>
    <title>Diversity-Enriched Option-Critic</title>
    <updated>2020-11-04T22:12:54Z</updated>
    <link href="https://arxiv.org/abs/2011.02565v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2011.02565v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Temporal abstraction allows reinforcement learning agents to represent knowledge and develop strategies over different temporal scales. The option-critic framework has been demonstrated to learn temporally extended actions, represented as options, end-to-end in a model-free setting. However, feasibility of option-critic remains limited due to two major challenges, multiple options adopting very similar behavior, or a shrinking set of task relevant options. These occurrences not only void the need for temporal abstraction, they also affect performance. In this paper, we tackle these problems by learning a diverse set of options. We introduce an information-theoretic intrinsic reward, which augments the task reward, as well as a novel termination objective, in order to encourage behavioral diversity in the option set. We show empirically that our proposed method is capable of learning options end-to-end on several discrete and continuous control tasks, outperforms option-critic by a wide margin. Furthermore, we show that our approach sustainably generates robust, reusable, reliable and interpretable options, in contrast to option-critic.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-11-04T22:12:54Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <arxiv:journal_ref>arXiv:2011.02565 [cs.AI]</arxiv:journal_ref>
    <author>
      <name>Anand Kamat</name>
    </author>
    <author>
      <name>Doina Precup</name>
    </author>
    <arxiv:doi>10.48550/arXiv.2011.02565</arxiv:doi>
    <link rel="related" href="https://doi.org/10.48550/arXiv.2011.02565" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.10475v1</id>
    <title>Can You Detect the Difference?</title>
    <updated>2025-07-14T16:55:57Z</updated>
    <link href="https://arxiv.org/abs/2507.10475v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2507.10475v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-07-14T16:55:57Z</published>
    <arxiv:comment>11 pages, 3 figures, 2 tables. Code and data: https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for AI-safety relevance</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>İsmail Tarım</name>
    </author>
    <author>
      <name>Aytuğ Onan</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cond-mat/0408190v2</id>
    <title>From spin glasses to hard satisfiable formulas</title>
    <updated>2012-10-17T15:32:28Z</updated>
    <link href="https://arxiv.org/abs/cond-mat/0408190v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/cond-mat/0408190v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We introduce a highly structured family of hard satisfiable 3-SAT formulas corresponding to an ordered spin-glass model from statistical physics. This model has provably "glassy" behavior; that is, it has many local optima with large energy barriers between them, so that local search algorithms get stuck and have difficulty finding the true ground state, i.e., the unique satisfying assignment. We test the hardness of our formulas with two Davis-Putnam solvers, Satz and zChaff, the recently introduced Survey Propagation (SP), and two local search algorithms, Walksat and Record-to-Record Travel (RRT). We compare our formulas to random 3-XOR-SAT formulas and to two other generators of hard satisfiable instances, the minimum disagreement parity formulas of Crawford et al., and Hirsch's hgen. For the complete solvers the running time of our formulas grows exponentially in sqrt(n), and exceeds that of random 3-XOR-SAT formulas for small problem sizes. SP is unable to solve our formulas with as few as 25 variables. For Walksat, our formulas appear to be harder than any other known generator of satisfiable instances. Finally, our formulas can be solved efficiently by RRT but only if the parameter d is tuned to the height of the barriers between local minima, and we use this parameter to measure the barrier heights in random 3-XOR-SAT formulas as well.</summary>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2004-08-09T21:54:07Z</published>
    <arxiv:comment>This is now somewhat out of date, but we thought it would make sense to cross-list to cs.AI as an example of hard SAT problems. The preliminary version (without results on RRT) appeared in SAT 2004</arxiv:comment>
    <arxiv:primary_category term="cond-mat.stat-mech"/>
    <author>
      <name>Haixia Jia</name>
    </author>
    <author>
      <name>Cristopher Moore</name>
    </author>
    <author>
      <name>Bart Selman</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.07143v2</id>
    <title>Identifying the Development and Application of Artificial Intelligence in Scientific Text</title>
    <updated>2020-05-28T15:35:17Z</updated>
    <link href="https://arxiv.org/abs/2002.07143v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2002.07143v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We describe a strategy for identifying the universe of research publications relevant to the application and development of artificial intelligence. The approach leverages the arXiv corpus of scientific preprints, in which authors choose subject tags for their papers from a set defined by editors. We compose a functional definition of AI relevance by learning these subjects from paper metadata, and then inferring the arXiv-subject labels of papers in larger corpora: Clarivate Web of Science, Digital Science Dimensions, and Microsoft Academic Graph. This yields predictive classification $F_1$ scores between .75 and .86 for Natural Language Processing (cs.CL), Computer Vision (cs.CV), and Robotics (cs.RO). For a single model that learns these and four other AI-relevant subjects (cs.AI, cs.LG, stat.ML, and cs.MA), we see precision of .83 and recall of .85. We evaluate the out-of-domain performance of our classifiers against other sources of topic information and predictions from alternative methods. We find that a supervised solution can generalize to identify publications that belong to the high-level fields of study represented on arXiv. This offers a method for identifying AI-relevant publications that updates at the pace of research output, without reliance on subject-matter experts for query development or labeling.</summary>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-02-17T18:58:59Z</published>
    <arxiv:comment>This revision expands our analysis in Section 5. We predict and evaluate labels for publications in Microsoft Academic Graph and Digital Science Dimensions, in addition to Clarivate Web of Science</arxiv:comment>
    <arxiv:primary_category term="cs.DL"/>
    <author>
      <name>James Dunham</name>
    </author>
    <author>
      <name>Jennifer Melot</name>
    </author>
    <author>
      <name>Dewey Murdick</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.04697v2</id>
    <title>The Existential Theory of the Reals with Summation Operators</title>
    <updated>2024-10-04T10:22:31Z</updated>
    <link href="https://arxiv.org/abs/2405.04697v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2405.04697v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>To characterize the computational complexity of satisfiability problems for probabilistic and causal reasoning within the Pearl's Causal Hierarchy, arXiv:2305.09508 [cs.AI] introduce a new natural class, named succ-$\exists$R. This class can be viewed as a succinct variant of the well-studied class $\exists$R based on the Existential Theory of the Reals (ETR). Analogously to $\exists$R, succ-$\exists$R is an intermediate class between NEXP and EXPSPACE, the exponential versions of NP and PSPACE. The main contributions of this work are threefold. Firstly, we characterize the class succ-$\exists$R in terms of nondeterministic real RAM machines and develop structural complexity theoretic results for real RAMs, including translation and hierarchy theorems. Notably, we demonstrate the separation of $\exists$R and succ-$\exists$R. Secondly, we examine the complexity of model checking and satisfiability of fragments of existential second-order logic and probabilistic independence logic. We show succ-$\exists$R- completeness of several of these problems, for which the best-known complexity lower and upper bounds were previously NEXP-hardness and EXPSPACE, respectively. Thirdly, while succ-$\exists$R is characterized in terms of ordinary (non-succinct) ETR instances enriched by exponential sums and a mechanism to index exponentially many variables, in this paper, we prove that when only exponential sums are added, the corresponding class $\exists$R^Σ is contained in PSPACE. We conjecture that this inclusion is strict, as this class is equivalent to adding a VNP-oracle to a polynomial time nondeterministic real RAM. Conversely, the addition of exponential products to ETR, yields PSPACE. Additionally, we study the satisfiability problem for probabilistic reasoning, with the additional requirement of a small model and prove that this problem is complete for $\exists$R^Σ.</summary>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-05-07T22:20:41Z</published>
    <arxiv:comment>ISAAC 2024</arxiv:comment>
    <arxiv:primary_category term="cs.CC"/>
    <author>
      <name>Markus Bläser</name>
    </author>
    <author>
      <name>Julian Dörfler</name>
    </author>
    <author>
      <name>Maciej Liskiewicz</name>
    </author>
    <author>
      <name>Benito van der Zander</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.09029v1</id>
    <title>Enhancing Data Integrity through Provenance Tracking in Semantic Web Frameworks</title>
    <updated>2025-01-12T16:13:27Z</updated>
    <link href="https://arxiv.org/abs/2501.09029v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2501.09029v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>This paper explores the integration of provenance tracking systems within the context of Semantic Web technologies to enhance data integrity in diverse operational environments. SURROUND Australia Pty Ltd demonstrates innovative applica-tions of the PROV Data Model (PROV-DM) and its Semantic Web variant, PROV-O, to systematically record and manage provenance information across multiple data processing domains. By employing RDF and Knowledge Graphs, SURROUND ad-dresses the critical challenges of shared entity identification and provenance granularity. The paper highlights the company's architecture for capturing comprehensive provenance data, en-abling robust validation, traceability, and knowledge inference. Through the examination of two projects, we illustrate how provenance mechanisms not only improve data reliability but also facilitate seamless integration across heterogeneous systems. Our findings underscore the importance of sophisticated provenance solutions in maintaining data integrity, serving as a reference for industry peers and academics engaged in provenance research and implementation.</summary>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-01-12T16:13:27Z</published>
    <arxiv:comment>This 10-page manuscript with 5 figures focuses on leveraging Semantic Web frameworks to enhance data integrity through provenance tracking. Intended for conference submission, it aligns with the cs.AI category, addressing knowledge representation, data modeling, and uncertainty in AI using advanced tools like PROV-DM and PROV-O</arxiv:comment>
    <arxiv:primary_category term="cs.CR"/>
    <author>
      <name>Nilesh Jain</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.16035v1</id>
    <title>Liars' Bench: Evaluating Lie Detectors for Language Models</title>
    <updated>2025-11-20T04:29:33Z</updated>
    <link href="https://arxiv.org/abs/2511.16035v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.16035v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-20T04:29:33Z</published>
    <arxiv:comment>*Kieron Kretschmar and Walter Laurito contributed equally to this work. 10 pages, 2 figures; plus appendix. Code at https://github.com/Cadenza-Labs/liars-bench and datasets at https://huggingface.co/datasets/Cadenza-Labs/liars-bench Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Kieron Kretschmar</name>
      <arxiv:affiliation>Cadenza Labs</arxiv:affiliation>
    </author>
    <author>
      <name>Walter Laurito</name>
      <arxiv:affiliation>Cadenza Labs</arxiv:affiliation>
      <arxiv:affiliation>FZI</arxiv:affiliation>
    </author>
    <author>
      <name>Sharan Maiya</name>
      <arxiv:affiliation>Cadenza Labs</arxiv:affiliation>
      <arxiv:affiliation>University of Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Samuel Marks</name>
      <arxiv:affiliation>Anthropic</arxiv:affiliation>
    </author>
  </entry>
</feed>
